[
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "argparse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "argparse",
        "description": "argparse",
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "shutil",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "shutil",
        "description": "shutil",
        "detail": "shutil",
        "documentation": {}
    },
    {
        "label": "logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging",
        "description": "logging",
        "detail": "logging",
        "documentation": {}
    },
    {
        "label": "hashlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "hashlib",
        "description": "hashlib",
        "detail": "hashlib",
        "documentation": {}
    },
    {
        "label": "pickle",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pickle",
        "description": "pickle",
        "detail": "pickle",
        "documentation": {}
    },
    {
        "label": "signal",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "signal",
        "description": "signal",
        "detail": "signal",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cv2",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "cv2",
        "description": "cv2",
        "detail": "cv2",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "yaml",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "yaml",
        "description": "yaml",
        "detail": "yaml",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "UnidentifiedImageError",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "UnidentifiedImageError",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "UnidentifiedImageError",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "UnidentifiedImageError",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "UnidentifiedImageError",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "torch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch",
        "description": "torch",
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "clip",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "clip",
        "description": "clip",
        "detail": "clip",
        "documentation": {}
    },
    {
        "label": "rawpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "rawpy",
        "description": "rawpy",
        "detail": "rawpy",
        "documentation": {}
    },
    {
        "label": "DBSCAN",
        "importPath": "sklearn.cluster",
        "description": "sklearn.cluster",
        "isExtraImport": true,
        "detail": "sklearn.cluster",
        "documentation": {}
    },
    {
        "label": "DBSCAN",
        "importPath": "sklearn.cluster",
        "description": "sklearn.cluster",
        "isExtraImport": true,
        "detail": "sklearn.cluster",
        "documentation": {}
    },
    {
        "label": "DBSCAN",
        "importPath": "sklearn.cluster",
        "description": "sklearn.cluster",
        "isExtraImport": true,
        "detail": "sklearn.cluster",
        "documentation": {}
    },
    {
        "label": "DBSCAN",
        "importPath": "sklearn.cluster",
        "description": "sklearn.cluster",
        "isExtraImport": true,
        "detail": "sklearn.cluster",
        "documentation": {}
    },
    {
        "label": "DBSCAN",
        "importPath": "sklearn.cluster",
        "description": "sklearn.cluster",
        "isExtraImport": true,
        "detail": "sklearn.cluster",
        "documentation": {}
    },
    {
        "label": "DBSCAN",
        "importPath": "sklearn.cluster",
        "description": "sklearn.cluster",
        "isExtraImport": true,
        "detail": "sklearn.cluster",
        "documentation": {}
    },
    {
        "label": "StandardScaler",
        "importPath": "sklearn.preprocessing",
        "description": "sklearn.preprocessing",
        "isExtraImport": true,
        "detail": "sklearn.preprocessing",
        "documentation": {}
    },
    {
        "label": "StandardScaler",
        "importPath": "sklearn.preprocessing",
        "description": "sklearn.preprocessing",
        "isExtraImport": true,
        "detail": "sklearn.preprocessing",
        "documentation": {}
    },
    {
        "label": "StandardScaler",
        "importPath": "sklearn.preprocessing",
        "description": "sklearn.preprocessing",
        "isExtraImport": true,
        "detail": "sklearn.preprocessing",
        "documentation": {}
    },
    {
        "label": "StandardScaler",
        "importPath": "sklearn.preprocessing",
        "description": "sklearn.preprocessing",
        "isExtraImport": true,
        "detail": "sklearn.preprocessing",
        "documentation": {}
    },
    {
        "label": "StandardScaler",
        "importPath": "sklearn.preprocessing",
        "description": "sklearn.preprocessing",
        "isExtraImport": true,
        "detail": "sklearn.preprocessing",
        "documentation": {}
    },
    {
        "label": "HDBSCAN",
        "importPath": "hdbscan",
        "description": "hdbscan",
        "isExtraImport": true,
        "detail": "hdbscan",
        "documentation": {}
    },
    {
        "label": "HDBSCAN",
        "importPath": "hdbscan",
        "description": "hdbscan",
        "isExtraImport": true,
        "detail": "hdbscan",
        "documentation": {}
    },
    {
        "label": "HDBSCAN",
        "importPath": "hdbscan",
        "description": "hdbscan",
        "isExtraImport": true,
        "detail": "hdbscan",
        "documentation": {}
    },
    {
        "label": "HDBSCAN",
        "importPath": "hdbscan",
        "description": "hdbscan",
        "isExtraImport": true,
        "detail": "hdbscan",
        "documentation": {}
    },
    {
        "label": "HDBSCAN",
        "importPath": "hdbscan",
        "description": "hdbscan",
        "isExtraImport": true,
        "detail": "hdbscan",
        "documentation": {}
    },
    {
        "label": "ThreadPoolExecutor",
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "isExtraImport": true,
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "as_completed",
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "isExtraImport": true,
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "ThreadPoolExecutor",
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "isExtraImport": true,
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "as_completed",
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "isExtraImport": true,
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "ThreadPoolExecutor",
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "isExtraImport": true,
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "as_completed",
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "isExtraImport": true,
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "ThreadPoolExecutor",
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "isExtraImport": true,
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "as_completed",
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "isExtraImport": true,
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "ThreadPoolExecutor",
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "isExtraImport": true,
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "as_completed",
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "isExtraImport": true,
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "threading",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "threading",
        "description": "threading",
        "detail": "threading",
        "documentation": {}
    },
    {
        "label": "get_image_embedding",
        "importPath": "main",
        "description": "main",
        "isExtraImport": true,
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "get_video_embedding",
        "importPath": "main",
        "description": "main",
        "isExtraImport": true,
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "extract_embedding",
        "importPath": "main",
        "description": "main",
        "isExtraImport": true,
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "MediaProcessor",
        "kind": 6,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "class MediaProcessor:\n    instance = None  # For global access in signal handler\n    def __init__(self, config: dict):\n        self.config = config\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.model, self.preprocess = self._load_model()\n        self.file_hashes = set()\n        self.processed_files: List[Path] = []\n        self.features: List[Tuple[np.ndarray, float]] = []\n        self.last_checkpoint_time = time.time()",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "save_progress",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def save_progress(processed_files: List[Path], features: List[Tuple[np.ndarray, float]]):\n    \"\"\"Save checkpoint progress to a pickle file.\"\"\"\n    try:\n        with open(CHECKPOINT_FILE, \"wb\") as pf:\n            pickle.dump({\"processed_files\": processed_files, \"features\": features}, pf)\n        logger.info(\"Checkpoint saved with %d files.\", len(processed_files))\n    except Exception as e:\n        logger.error(\"Failed to save checkpoint: %s\", e)\ndef signal_handler(signum, frame):\n    \"\"\"On termination, save progress before exiting.\"\"\"",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "signal_handler",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def signal_handler(signum, frame):\n    \"\"\"On termination, save progress before exiting.\"\"\"\n    logger.info(\"Termination signal received; saving checkpoint before exit.\")\n    if MediaProcessor.instance is not None:\n        save_progress(MediaProcessor.instance.processed_files, MediaProcessor.instance.features)\n    sys.exit(0)\n# Register termination signal handlers.\nsignal.signal(signal.SIGINT, signal_handler)\nsignal.signal(signal.SIGTERM, signal_handler)\nclass MediaProcessor:",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "load_config",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def load_config(config_path: Path) -> dict:\n    \"\"\"Load configuration from a YAML file, or return the default config.\"\"\"\n    if config_path.exists():\n        try:\n            with open(config_path) as f:\n                loaded_config = yaml.safe_load(f)\n                return loaded_config\n        except Exception as e:\n            logger.error(\"Failed to load configuration file: %s\", e)\n    return DEFAULT_CONFIG",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "IMAGE_EXTENSIONS",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "IMAGE_EXTENSIONS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".gif\", \".tiff\"}\nVIDEO_EXTENSIONS = {\".mp4\", \".mov\", \".avi\", \".mkv\", \".wmv\", \".flv\"}\nRAW_EXTENSIONS   = {\".raw\", \".cr2\", \".nef\", \".dng\", \".arw\", \".rw2\"}\n# Global flags (updated via CLI)\nprocess_large_files = False\nfix_dates = False\n# --- Logging Setup ---\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "VIDEO_EXTENSIONS",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "VIDEO_EXTENSIONS = {\".mp4\", \".mov\", \".avi\", \".mkv\", \".wmv\", \".flv\"}\nRAW_EXTENSIONS   = {\".raw\", \".cr2\", \".nef\", \".dng\", \".arw\", \".rw2\"}\n# Global flags (updated via CLI)\nprocess_large_files = False\nfix_dates = False\n# --- Logging Setup ---\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    handlers=[logging.FileHandler(\"media_sorter.log\"), logging.StreamHandler()]",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "process_large_files",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "process_large_files = False\nfix_dates = False\n# --- Logging Setup ---\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    handlers=[logging.FileHandler(\"media_sorter.log\"), logging.StreamHandler()]\n)\nlogger = logging.getLogger(__name__)\nerror_handler = logging.FileHandler(\"error.log\", mode=\"a\")",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "fix_dates",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "fix_dates = False\n# --- Logging Setup ---\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    handlers=[logging.FileHandler(\"media_sorter.log\"), logging.StreamHandler()]\n)\nlogger = logging.getLogger(__name__)\nerror_handler = logging.FileHandler(\"error.log\", mode=\"a\")\nerror_handler.setLevel(logging.WARNING)",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "logger = logging.getLogger(__name__)\nerror_handler = logging.FileHandler(\"error.log\", mode=\"a\")\nerror_handler.setLevel(logging.WARNING)\nerror_formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\nerror_handler.setFormatter(error_formatter)\nlogger.addHandler(error_handler)\nDEFAULT_CONFIG = {\n    'clustering': {\n        'algorithm': 'hdbscan',  # 'dbscan' or 'hdbscan'\n        'eps': 0.15,",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "error_handler",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "error_handler = logging.FileHandler(\"error.log\", mode=\"a\")\nerror_handler.setLevel(logging.WARNING)\nerror_formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\nerror_handler.setFormatter(error_formatter)\nlogger.addHandler(error_handler)\nDEFAULT_CONFIG = {\n    'clustering': {\n        'algorithm': 'hdbscan',  # 'dbscan' or 'hdbscan'\n        'eps': 0.15,\n        'min_samples': 3,",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "error_formatter",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "error_formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\nerror_handler.setFormatter(error_formatter)\nlogger.addHandler(error_handler)\nDEFAULT_CONFIG = {\n    'clustering': {\n        'algorithm': 'hdbscan',  # 'dbscan' or 'hdbscan'\n        'eps': 0.15,\n        'min_samples': 3,\n        'visual_weight': 0.7,\n        'temporal_weight': 0.3",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "DEFAULT_CONFIG",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "DEFAULT_CONFIG = {\n    'clustering': {\n        'algorithm': 'hdbscan',  # 'dbscan' or 'hdbscan'\n        'eps': 0.15,\n        'min_samples': 3,\n        'visual_weight': 0.7,\n        'temporal_weight': 0.3\n    },\n    'processing': {\n        'batch_size': 32,",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "CHECKPOINT_FILE",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "CHECKPOINT_FILE = \"progress.pkl\"\ndef save_progress(processed_files: List[Path], features: List[Tuple[np.ndarray, float]]):\n    \"\"\"Save checkpoint progress to a pickle file.\"\"\"\n    try:\n        with open(CHECKPOINT_FILE, \"wb\") as pf:\n            pickle.dump({\"processed_files\": processed_files, \"features\": features}, pf)\n        logger.info(\"Checkpoint saved with %d files.\", len(processed_files))\n    except Exception as e:\n        logger.error(\"Failed to save checkpoint: %s\", e)\ndef signal_handler(signum, frame):",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "MediaProcessor",
        "kind": 6,
        "importPath": "media_scene_sorter",
        "description": "media_scene_sorter",
        "peekOfCode": "class MediaProcessor:\n    def __init__(self, config: dict):\n        self.config = config\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.model, self.preprocess = self._load_model()\n        self.file_hashes = set()\n        # Lock for thread safety when calling the CLIP model\n        self.model_lock = None\n    def _load_model(self):\n        \"\"\"Load the CLIP model.\"\"\"",
        "detail": "media_scene_sorter",
        "documentation": {}
    },
    {
        "label": "load_config",
        "kind": 2,
        "importPath": "media_scene_sorter",
        "description": "media_scene_sorter",
        "peekOfCode": "def load_config(config_path: Path) -> dict:\n    \"\"\"Load configuration from a YAML file if it exists; otherwise, return the default config.\"\"\"\n    if config_path.exists():\n        try:\n            with open(config_path) as f:\n                loaded_config = yaml.safe_load(f)\n                return loaded_config\n        except Exception as e:\n            logger.error(\"Failed to load configuration file: %s\", e)\n    return DEFAULT_CONFIG",
        "detail": "media_scene_sorter",
        "documentation": {}
    },
    {
        "label": "IMAGE_EXTENSIONS",
        "kind": 5,
        "importPath": "media_scene_sorter",
        "description": "media_scene_sorter",
        "peekOfCode": "IMAGE_EXTENSIONS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".gif\", \".tiff\"}\nVIDEO_EXTENSIONS = {\".mp4\", \".mov\", \".avi\", \".mkv\", \".wmv\", \".flv\"}\nRAW_EXTENSIONS   = {\".raw\", \".cr2\", \".nef\", \".dng\", \".arw\", \".rw2\"}\n# Global flag to control processing of large files (set via CLI)\nprocess_large_files = False\n# --- Logging Setup ---\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    handlers=[",
        "detail": "media_scene_sorter",
        "documentation": {}
    },
    {
        "label": "VIDEO_EXTENSIONS",
        "kind": 5,
        "importPath": "media_scene_sorter",
        "description": "media_scene_sorter",
        "peekOfCode": "VIDEO_EXTENSIONS = {\".mp4\", \".mov\", \".avi\", \".mkv\", \".wmv\", \".flv\"}\nRAW_EXTENSIONS   = {\".raw\", \".cr2\", \".nef\", \".dng\", \".arw\", \".rw2\"}\n# Global flag to control processing of large files (set via CLI)\nprocess_large_files = False\n# --- Logging Setup ---\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    handlers=[\n        logging.FileHandler(\"media_sorter.log\"),",
        "detail": "media_scene_sorter",
        "documentation": {}
    },
    {
        "label": "process_large_files",
        "kind": 5,
        "importPath": "media_scene_sorter",
        "description": "media_scene_sorter",
        "peekOfCode": "process_large_files = False\n# --- Logging Setup ---\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    handlers=[\n        logging.FileHandler(\"media_sorter.log\"),\n        logging.StreamHandler()\n    ]\n)",
        "detail": "media_scene_sorter",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "media_scene_sorter",
        "description": "media_scene_sorter",
        "peekOfCode": "logger = logging.getLogger(__name__)\nerror_handler = logging.FileHandler(\"error.log\", mode=\"a\")\nerror_handler.setLevel(logging.WARNING)\nerror_formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\nerror_handler.setFormatter(error_formatter)\nlogger.addHandler(error_handler)\n# Default configuration\nDEFAULT_CONFIG = {\n    'clustering': {\n        'algorithm': 'hdbscan',  # Options: 'dbscan' or 'hdbscan'",
        "detail": "media_scene_sorter",
        "documentation": {}
    },
    {
        "label": "error_handler",
        "kind": 5,
        "importPath": "media_scene_sorter",
        "description": "media_scene_sorter",
        "peekOfCode": "error_handler = logging.FileHandler(\"error.log\", mode=\"a\")\nerror_handler.setLevel(logging.WARNING)\nerror_formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\nerror_handler.setFormatter(error_formatter)\nlogger.addHandler(error_handler)\n# Default configuration\nDEFAULT_CONFIG = {\n    'clustering': {\n        'algorithm': 'hdbscan',  # Options: 'dbscan' or 'hdbscan'\n        'eps': 0.19, # was 0.15",
        "detail": "media_scene_sorter",
        "documentation": {}
    },
    {
        "label": "error_formatter",
        "kind": 5,
        "importPath": "media_scene_sorter",
        "description": "media_scene_sorter",
        "peekOfCode": "error_formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\nerror_handler.setFormatter(error_formatter)\nlogger.addHandler(error_handler)\n# Default configuration\nDEFAULT_CONFIG = {\n    'clustering': {\n        'algorithm': 'hdbscan',  # Options: 'dbscan' or 'hdbscan'\n        'eps': 0.19, # was 0.15\n        'min_samples': 3,\n        'visual_weight': 0.8, # .7",
        "detail": "media_scene_sorter",
        "documentation": {}
    },
    {
        "label": "DEFAULT_CONFIG",
        "kind": 5,
        "importPath": "media_scene_sorter",
        "description": "media_scene_sorter",
        "peekOfCode": "DEFAULT_CONFIG = {\n    'clustering': {\n        'algorithm': 'hdbscan',  # Options: 'dbscan' or 'hdbscan'\n        'eps': 0.19, # was 0.15\n        'min_samples': 3,\n        'visual_weight': 0.8, # .7\n        'temporal_weight': 0.2 # .3\n    },\n    'processing': {\n        'batch_size': 32,",
        "detail": "media_scene_sorter",
        "documentation": {}
    },
    {
        "label": "MediaProcessor",
        "kind": 6,
        "importPath": "multithread_version",
        "description": "multithread_version",
        "peekOfCode": "class MediaProcessor:\n    def __init__(self, config: dict):\n        self.config = config\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.model, self.preprocess = self._load_model()\n        self.file_hashes = set()\n        # Lock to ensure thread safety when calling the CLIP model\n        self.model_lock = threading.Lock()\n    def _load_model(self):\n        \"\"\"Load CLIP model with error handling.\"\"\"",
        "detail": "multithread_version",
        "documentation": {}
    },
    {
        "label": "load_config",
        "kind": 2,
        "importPath": "multithread_version",
        "description": "multithread_version",
        "peekOfCode": "def load_config(config_path: Path) -> dict:\n    \"\"\"Load configuration from a YAML file, or return the default config.\"\"\"\n    if config_path.exists():\n        try:\n            with open(config_path) as f:\n                loaded_config = yaml.safe_load(f)\n                return loaded_config\n        except Exception as e:\n            logger.error(\"Failed to load configuration file: %s\", e)\n    return DEFAULT_CONFIG",
        "detail": "multithread_version",
        "documentation": {}
    },
    {
        "label": "IMAGE_EXTENSIONS",
        "kind": 5,
        "importPath": "multithread_version",
        "description": "multithread_version",
        "peekOfCode": "IMAGE_EXTENSIONS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".gif\", \".tiff\"}\nVIDEO_EXTENSIONS = {\".mp4\", \".mov\", \".avi\", \".mkv\", \".wmv\", \".flv\"}\nRAW_EXTENSIONS   = {\".raw\", \".cr2\", \".nef\", \".dng\", \".arw\", \".rw2\"}\n# Global flag to control processing of large files.\n# Default is False: large files are skipped.\nprocess_large_files = False  # This will be updated via a CLI parameter.\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",",
        "detail": "multithread_version",
        "documentation": {}
    },
    {
        "label": "VIDEO_EXTENSIONS",
        "kind": 5,
        "importPath": "multithread_version",
        "description": "multithread_version",
        "peekOfCode": "VIDEO_EXTENSIONS = {\".mp4\", \".mov\", \".avi\", \".mkv\", \".wmv\", \".flv\"}\nRAW_EXTENSIONS   = {\".raw\", \".cr2\", \".nef\", \".dng\", \".arw\", \".rw2\"}\n# Global flag to control processing of large files.\n# Default is False: large files are skipped.\nprocess_large_files = False  # This will be updated via a CLI parameter.\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    handlers=[",
        "detail": "multithread_version",
        "documentation": {}
    },
    {
        "label": "process_large_files",
        "kind": 5,
        "importPath": "multithread_version",
        "description": "multithread_version",
        "peekOfCode": "process_large_files = False  # This will be updated via a CLI parameter.\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    handlers=[\n        logging.FileHandler(\"media_sorter.log\"),\n        logging.StreamHandler()\n    ]\n)",
        "detail": "multithread_version",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "multithread_version",
        "description": "multithread_version",
        "peekOfCode": "logger = logging.getLogger(__name__)\nDEFAULT_CONFIG = {\n    'clustering': {\n        'algorithm': 'hdbscan',  # 'dbscan' or 'hdbscan'\n        'eps': 0.15,\n        'min_samples': 3,\n        'visual_weight': 0.7,\n        'temporal_weight': 0.3\n    },\n    'processing': {",
        "detail": "multithread_version",
        "documentation": {}
    },
    {
        "label": "DEFAULT_CONFIG",
        "kind": 5,
        "importPath": "multithread_version",
        "description": "multithread_version",
        "peekOfCode": "DEFAULT_CONFIG = {\n    'clustering': {\n        'algorithm': 'hdbscan',  # 'dbscan' or 'hdbscan'\n        'eps': 0.15,\n        'min_samples': 3,\n        'visual_weight': 0.7,\n        'temporal_weight': 0.3\n    },\n    'processing': {\n        'batch_size': 32,",
        "detail": "multithread_version",
        "documentation": {}
    },
    {
        "label": "MediaProcessor",
        "kind": 6,
        "importPath": "multithread_version2",
        "description": "multithread_version2",
        "peekOfCode": "class MediaProcessor:\n    def __init__(self, config: dict):\n        self.config = config\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.model, self.preprocess = self._load_model()\n        self.file_hashes = set()\n        # Lock to ensure thread safety when calling the CLIP model\n        self.model_lock = threading.Lock()\n    def _load_model(self):\n        \"\"\"Load CLIP model with error handling.\"\"\"",
        "detail": "multithread_version2",
        "documentation": {}
    },
    {
        "label": "load_config",
        "kind": 2,
        "importPath": "multithread_version2",
        "description": "multithread_version2",
        "peekOfCode": "def load_config(config_path: Path) -> dict:\n    \"\"\"Load configuration from a YAML file, or return the default config.\"\"\"\n    if config_path.exists():\n        try:\n            with open(config_path) as f:\n                loaded_config = yaml.safe_load(f)\n                return loaded_config\n        except Exception as e:\n            logger.error(\"Failed to load configuration file: %s\", e)\n    return DEFAULT_CONFIG",
        "detail": "multithread_version2",
        "documentation": {}
    },
    {
        "label": "IMAGE_EXTENSIONS",
        "kind": 5,
        "importPath": "multithread_version2",
        "description": "multithread_version2",
        "peekOfCode": "IMAGE_EXTENSIONS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".gif\", \".tiff\"}\nVIDEO_EXTENSIONS = {\".mp4\", \".mov\", \".avi\", \".mkv\", \".wmv\", \".flv\"}\nRAW_EXTENSIONS   = {\".raw\", \".cr2\", \".nef\", \".dng\", \".arw\", \".rw2\"}\n# Global flag to control processing of large files.\n# Default is False: large files are skipped.\nprocess_large_files = False  # Updated via CLI\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",",
        "detail": "multithread_version2",
        "documentation": {}
    },
    {
        "label": "VIDEO_EXTENSIONS",
        "kind": 5,
        "importPath": "multithread_version2",
        "description": "multithread_version2",
        "peekOfCode": "VIDEO_EXTENSIONS = {\".mp4\", \".mov\", \".avi\", \".mkv\", \".wmv\", \".flv\"}\nRAW_EXTENSIONS   = {\".raw\", \".cr2\", \".nef\", \".dng\", \".arw\", \".rw2\"}\n# Global flag to control processing of large files.\n# Default is False: large files are skipped.\nprocess_large_files = False  # Updated via CLI\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    handlers=[",
        "detail": "multithread_version2",
        "documentation": {}
    },
    {
        "label": "process_large_files",
        "kind": 5,
        "importPath": "multithread_version2",
        "description": "multithread_version2",
        "peekOfCode": "process_large_files = False  # Updated via CLI\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    handlers=[\n        logging.FileHandler(\"media_sorter.log\"),\n        logging.StreamHandler()\n    ]\n)",
        "detail": "multithread_version2",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "multithread_version2",
        "description": "multithread_version2",
        "peekOfCode": "logger = logging.getLogger(__name__)\nDEFAULT_CONFIG = {\n    'clustering': {\n        'algorithm': 'hdbscan',  # 'dbscan' or 'hdbscan'\n        'eps': 0.15,\n        'min_samples': 3,\n        'visual_weight': 0.7,\n        'temporal_weight': 0.3\n    },\n    'processing': {",
        "detail": "multithread_version2",
        "documentation": {}
    },
    {
        "label": "DEFAULT_CONFIG",
        "kind": 5,
        "importPath": "multithread_version2",
        "description": "multithread_version2",
        "peekOfCode": "DEFAULT_CONFIG = {\n    'clustering': {\n        'algorithm': 'hdbscan',  # 'dbscan' or 'hdbscan'\n        'eps': 0.15,\n        'min_samples': 3,\n        'visual_weight': 0.7,\n        'temporal_weight': 0.3\n    },\n    'processing': {\n        'batch_size': 32,",
        "detail": "multithread_version2",
        "documentation": {}
    },
    {
        "label": "MediaProcessor",
        "kind": 6,
        "importPath": "multithread_withdashboard",
        "description": "multithread_withdashboard",
        "peekOfCode": "class MediaProcessor:\n    def __init__(self, config: dict):\n        self.config = config\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.model, self.preprocess = self._load_model()\n        self.file_hashes = set()\n        # Lock for thread safety when calling the CLIP model\n        self.model_lock = threading.Lock()\n    def _load_model(self):\n        \"\"\"Load CLIP model with error handling.\"\"\"",
        "detail": "multithread_withdashboard",
        "documentation": {}
    },
    {
        "label": "dashboard_updater",
        "kind": 2,
        "importPath": "multithread_withdashboard",
        "description": "multithread_withdashboard",
        "peekOfCode": "def dashboard_updater():\n    \"\"\"Thread function that writes the current video dashboard to 'dashboard.txt' every second.\"\"\"\n    import time\n    while True:\n        lines = [\"| Video File | Batch # | Frames in Batch | Std Dev |\"]\n        lines.append(\"|------------|---------|-----------------|---------|\")\n        with dashboard_lock:\n            for video, batches in video_dashboard.items():\n                for batch in batches:\n                    lines.append(f\"| {video} | {batch['batch_num']} | {batch['frame_count']} | {batch['stdev']:.4f} |\")",
        "detail": "multithread_withdashboard",
        "documentation": {}
    },
    {
        "label": "load_config",
        "kind": 2,
        "importPath": "multithread_withdashboard",
        "description": "multithread_withdashboard",
        "peekOfCode": "def load_config(config_path: Path) -> dict:\n    \"\"\"Load configuration from a YAML file, or return the default config.\"\"\"\n    if config_path.exists():\n        try:\n            with open(config_path) as f:\n                loaded_config = yaml.safe_load(f)\n                return loaded_config\n        except Exception as e:\n            logger.error(\"Failed to load configuration file: %s\", e)\n    return DEFAULT_CONFIG",
        "detail": "multithread_withdashboard",
        "documentation": {}
    },
    {
        "label": "IMAGE_EXTENSIONS",
        "kind": 5,
        "importPath": "multithread_withdashboard",
        "description": "multithread_withdashboard",
        "peekOfCode": "IMAGE_EXTENSIONS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".gif\", \".tiff\"}\nVIDEO_EXTENSIONS = {\".mp4\", \".mov\", \".avi\", \".mkv\", \".wmv\", \".flv\"}\nRAW_EXTENSIONS   = {\".raw\", \".cr2\", \".nef\", \".dng\", \".arw\", \".rw2\"}\n# Global flag to control processing of large files.\n# Default is False: large files are skipped.\nprocess_large_files = False  # Updated via CLI\n# --- Logging Setup ---\n# Basic log (both to console and file)\nlogging.basicConfig(\n    level=logging.INFO,",
        "detail": "multithread_withdashboard",
        "documentation": {}
    },
    {
        "label": "VIDEO_EXTENSIONS",
        "kind": 5,
        "importPath": "multithread_withdashboard",
        "description": "multithread_withdashboard",
        "peekOfCode": "VIDEO_EXTENSIONS = {\".mp4\", \".mov\", \".avi\", \".mkv\", \".wmv\", \".flv\"}\nRAW_EXTENSIONS   = {\".raw\", \".cr2\", \".nef\", \".dng\", \".arw\", \".rw2\"}\n# Global flag to control processing of large files.\n# Default is False: large files are skipped.\nprocess_large_files = False  # Updated via CLI\n# --- Logging Setup ---\n# Basic log (both to console and file)\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",",
        "detail": "multithread_withdashboard",
        "documentation": {}
    },
    {
        "label": "process_large_files",
        "kind": 5,
        "importPath": "multithread_withdashboard",
        "description": "multithread_withdashboard",
        "peekOfCode": "process_large_files = False  # Updated via CLI\n# --- Logging Setup ---\n# Basic log (both to console and file)\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    handlers=[\n        logging.FileHandler(\"media_sorter.log\"),\n        logging.StreamHandler()\n    ]",
        "detail": "multithread_withdashboard",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "multithread_withdashboard",
        "description": "multithread_withdashboard",
        "peekOfCode": "logger = logging.getLogger(__name__)\n# Error log file for warnings/errors\nerror_handler = logging.FileHandler(\"error.log\", mode=\"a\")\nerror_handler.setLevel(logging.WARNING)\nerror_formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\nerror_handler.setFormatter(error_formatter)\nlogger.addHandler(error_handler)\nDEFAULT_CONFIG = {\n    'clustering': {\n        'algorithm': 'hdbscan',  # 'dbscan' or 'hdbscan'",
        "detail": "multithread_withdashboard",
        "documentation": {}
    },
    {
        "label": "error_handler",
        "kind": 5,
        "importPath": "multithread_withdashboard",
        "description": "multithread_withdashboard",
        "peekOfCode": "error_handler = logging.FileHandler(\"error.log\", mode=\"a\")\nerror_handler.setLevel(logging.WARNING)\nerror_formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\nerror_handler.setFormatter(error_formatter)\nlogger.addHandler(error_handler)\nDEFAULT_CONFIG = {\n    'clustering': {\n        'algorithm': 'hdbscan',  # 'dbscan' or 'hdbscan'\n        'eps': 0.15,\n        'min_samples': 3,",
        "detail": "multithread_withdashboard",
        "documentation": {}
    },
    {
        "label": "error_formatter",
        "kind": 5,
        "importPath": "multithread_withdashboard",
        "description": "multithread_withdashboard",
        "peekOfCode": "error_formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\nerror_handler.setFormatter(error_formatter)\nlogger.addHandler(error_handler)\nDEFAULT_CONFIG = {\n    'clustering': {\n        'algorithm': 'hdbscan',  # 'dbscan' or 'hdbscan'\n        'eps': 0.15,\n        'min_samples': 3,\n        'visual_weight': 0.7,\n        'temporal_weight': 0.3",
        "detail": "multithread_withdashboard",
        "documentation": {}
    },
    {
        "label": "DEFAULT_CONFIG",
        "kind": 5,
        "importPath": "multithread_withdashboard",
        "description": "multithread_withdashboard",
        "peekOfCode": "DEFAULT_CONFIG = {\n    'clustering': {\n        'algorithm': 'hdbscan',  # 'dbscan' or 'hdbscan'\n        'eps': 0.15,\n        'min_samples': 3,\n        'visual_weight': 0.7,\n        'temporal_weight': 0.3\n    },\n    'processing': {\n        'batch_size': 32,",
        "detail": "multithread_withdashboard",
        "documentation": {}
    },
    {
        "label": "video_dashboard",
        "kind": 5,
        "importPath": "multithread_withdashboard",
        "description": "multithread_withdashboard",
        "peekOfCode": "video_dashboard = {}\ndashboard_lock = threading.Lock()\ndef dashboard_updater():\n    \"\"\"Thread function that writes the current video dashboard to 'dashboard.txt' every second.\"\"\"\n    import time\n    while True:\n        lines = [\"| Video File | Batch # | Frames in Batch | Std Dev |\"]\n        lines.append(\"|------------|---------|-----------------|---------|\")\n        with dashboard_lock:\n            for video, batches in video_dashboard.items():",
        "detail": "multithread_withdashboard",
        "documentation": {}
    },
    {
        "label": "dashboard_lock",
        "kind": 5,
        "importPath": "multithread_withdashboard",
        "description": "multithread_withdashboard",
        "peekOfCode": "dashboard_lock = threading.Lock()\ndef dashboard_updater():\n    \"\"\"Thread function that writes the current video dashboard to 'dashboard.txt' every second.\"\"\"\n    import time\n    while True:\n        lines = [\"| Video File | Batch # | Frames in Batch | Std Dev |\"]\n        lines.append(\"|------------|---------|-----------------|---------|\")\n        with dashboard_lock:\n            for video, batches in video_dashboard.items():\n                for batch in batches:",
        "detail": "multithread_withdashboard",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "version2",
        "description": "version2",
        "peekOfCode": "def main():\n    # Gather all media files from the input folder and subfolders\n    file_paths = []\n    for root, _, files in tqdm(os.walk(INPUT_FOLDER)):\n            for file in files:\n                if file.startswith(\"._\"):  #? Skip hidden files\n                    continue  # Go to the next file in the loop\n                # print(f'ext: {os.path.splitext(file)}')\n                ext = os.path.splitext(file)[1]\n                ext = ext.lower() #? does this fix the error?",
        "detail": "version2",
        "documentation": {}
    },
    {
        "label": "INPUT_FOLDER",
        "kind": 5,
        "importPath": "version2",
        "description": "version2",
        "peekOfCode": "INPUT_FOLDER = \"/Volumes/BigBoy/portugal trip 2025\"  # Folder containing unclassified photos and videos\nOUTPUT_FOLDER = \"/Volumes/BigBoy/sorted_media\"  # Output directory where files are sorted by scene\n# DBSCAN parameters (adjust these for your data)\nEPS = 0.14  # Maximum distance between samples for them to be considered in the same neighborhood\nMIN_SAMPLES = 2  # Minimum number of samples required in a neighborhood to form a cluster\nVIDEO_FRAME_INTERVAL = 60  # seconds between key frames for video processing\n# Allowed file extensions for images and videos\nIMAGE_EXTENSIONS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\", \".dng\"}  # note - dng not included in initial code, may cause issues.\nVIDEO_EXTENSIONS = {\".mp4\", \".avi\", \".mov\", \".mkv\"}\n# -------------",
        "detail": "version2",
        "documentation": {}
    },
    {
        "label": "OUTPUT_FOLDER",
        "kind": 5,
        "importPath": "version2",
        "description": "version2",
        "peekOfCode": "OUTPUT_FOLDER = \"/Volumes/BigBoy/sorted_media\"  # Output directory where files are sorted by scene\n# DBSCAN parameters (adjust these for your data)\nEPS = 0.14  # Maximum distance between samples for them to be considered in the same neighborhood\nMIN_SAMPLES = 2  # Minimum number of samples required in a neighborhood to form a cluster\nVIDEO_FRAME_INTERVAL = 60  # seconds between key frames for video processing\n# Allowed file extensions for images and videos\nIMAGE_EXTENSIONS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\", \".dng\"}  # note - dng not included in initial code, may cause issues.\nVIDEO_EXTENSIONS = {\".mp4\", \".avi\", \".mov\", \".mkv\"}\n# -------------\n# Setup device and load the CLIP model",
        "detail": "version2",
        "documentation": {}
    },
    {
        "label": "EPS",
        "kind": 5,
        "importPath": "version2",
        "description": "version2",
        "peekOfCode": "EPS = 0.14  # Maximum distance between samples for them to be considered in the same neighborhood\nMIN_SAMPLES = 2  # Minimum number of samples required in a neighborhood to form a cluster\nVIDEO_FRAME_INTERVAL = 60  # seconds between key frames for video processing\n# Allowed file extensions for images and videos\nIMAGE_EXTENSIONS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\", \".dng\"}  # note - dng not included in initial code, may cause issues.\nVIDEO_EXTENSIONS = {\".mp4\", \".avi\", \".mov\", \".mkv\"}\n# -------------\n# Setup device and load the CLIP model\n# -------------\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"",
        "detail": "version2",
        "documentation": {}
    },
    {
        "label": "MIN_SAMPLES",
        "kind": 5,
        "importPath": "version2",
        "description": "version2",
        "peekOfCode": "MIN_SAMPLES = 2  # Minimum number of samples required in a neighborhood to form a cluster\nVIDEO_FRAME_INTERVAL = 60  # seconds between key frames for video processing\n# Allowed file extensions for images and videos\nIMAGE_EXTENSIONS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\", \".dng\"}  # note - dng not included in initial code, may cause issues.\nVIDEO_EXTENSIONS = {\".mp4\", \".avi\", \".mov\", \".mkv\"}\n# -------------\n# Setup device and load the CLIP model\n# -------------\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel, preprocess = clip.load(\"ViT-B/32\", device=device)",
        "detail": "version2",
        "documentation": {}
    },
    {
        "label": "VIDEO_FRAME_INTERVAL",
        "kind": 5,
        "importPath": "version2",
        "description": "version2",
        "peekOfCode": "VIDEO_FRAME_INTERVAL = 60  # seconds between key frames for video processing\n# Allowed file extensions for images and videos\nIMAGE_EXTENSIONS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\", \".dng\"}  # note - dng not included in initial code, may cause issues.\nVIDEO_EXTENSIONS = {\".mp4\", \".avi\", \".mov\", \".mkv\"}\n# -------------\n# Setup device and load the CLIP model\n# -------------\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel, preprocess = clip.load(\"ViT-B/32\", device=device)\nmodel.eval()",
        "detail": "version2",
        "documentation": {}
    },
    {
        "label": "IMAGE_EXTENSIONS",
        "kind": 5,
        "importPath": "version2",
        "description": "version2",
        "peekOfCode": "IMAGE_EXTENSIONS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\", \".dng\"}  # note - dng not included in initial code, may cause issues.\nVIDEO_EXTENSIONS = {\".mp4\", \".avi\", \".mov\", \".mkv\"}\n# -------------\n# Setup device and load the CLIP model\n# -------------\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel, preprocess = clip.load(\"ViT-B/32\", device=device)\nmodel.eval()\n# -------------\n# Functions to process media files",
        "detail": "version2",
        "documentation": {}
    },
    {
        "label": "VIDEO_EXTENSIONS",
        "kind": 5,
        "importPath": "version2",
        "description": "version2",
        "peekOfCode": "VIDEO_EXTENSIONS = {\".mp4\", \".avi\", \".mov\", \".mkv\"}\n# -------------\n# Setup device and load the CLIP model\n# -------------\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel, preprocess = clip.load(\"ViT-B/32\", device=device)\nmodel.eval()\n# -------------\n# Functions to process media files\n# -------------",
        "detail": "version2",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 5,
        "importPath": "version2",
        "description": "version2",
        "peekOfCode": "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel, preprocess = clip.load(\"ViT-B/32\", device=device)\nmodel.eval()\n# -------------\n# Functions to process media files\n# -------------\n# ... (get_image_embedding, get_video_embedding, extract_embedding remain the same)\nfrom main import get_image_embedding, get_video_embedding, extract_embedding #note: these are imported from the other file main.py\n# -------------\n# Main processing",
        "detail": "version2",
        "documentation": {}
    }
]